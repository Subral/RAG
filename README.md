# ğŸ§  RAG (Retrieval-Augmented Generation) Application

This project implements a **Retrieval-Augmented Generation (RAG)** pipeline using **Ollama** for local LLM and embedding inference. The app retrieves relevant context from a document store and generates intelligent responses using a fine-tuned language model.

---

## ğŸš€ Features

* ğŸ” **Context-Aware Retrieval** using the `bge-base-en-v1.5` embedding model
* ğŸ’¬ **Natural Language Generation** powered by `Llama-3.2-1B-Instruct`
* ğŸ§© **Local Execution** â€” fully offline, no external API required
* âš¡ **Fast and Lightweight** â€” ideal for local experimentation and private deployments

---

## ğŸ› ï¸ Installation

### 1. Clone the Repository

```bash
git clone https://github.com/subral/RAG.git
cd RAG
```

### 2. Install Ollama

Download and install **Ollama** from the official site:
ğŸ‘‰ [https://ollama.com/download](https://ollama.com/download)

---

## ğŸ“¦ Model Setup

After installing Ollama, pull the required models:

### ğŸ§© Embedding Model

```bash
ollama pull hf.co/CompendiumLabs/bge-base-en-v1.5-gguf
```

Model Source: [CompendiumLabs/bge-base-en-v1.5-gguf](https://huggingface.co/CompendiumLabs/bge-base-en-v1.5-gguf)

### ğŸ§  Language Model

```bash
ollama pull hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF
```

Model Source: [bartowski/Llama-3.2-1B-Instruct-GGUF](https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF)

---

## ğŸ§© Python Dependencies

Install the required Python packages:

```bash
pip install -r requirements.txt
```

---

## â–¶ï¸ Run the Application

Start the app with:

```bash
python app.py
```

Then open your browser at:

```
http://localhost:5000
```

---

## ğŸ§  How It Works

1. **User Query:** The user submits a question or prompt.
2. **Retriever:** The system embeds the query using the embedding model and retrieves the most relevant chunks.
3. **Generator:** The language model uses the retrieved context to generate a factual and coherent response.

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ app.py                 # Main application file
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ data/                  # Folder for documents or datasets
â”œâ”€â”€ models/                # (Optional) Custom models or configs
â””â”€â”€ README.md              # Project documentation
```

---

## âš™ï¸ Requirements

* Python 3.12
* Ollama installed and configured
* Internet connection (for first-time model download)

---

## ğŸ§‘â€ğŸ’» Author

**Subral Jaiswal**
ğŸ’¼ GitHub: [@yourusername](https://github.com/subral)
ğŸ“§ Email: [your.email@example.com](subraljaiswal6@gmail.com)

---

## ğŸª¶ License

This project is licensed under the [MIT License](LICENSE).
